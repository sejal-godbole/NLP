{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQskgaMPXMg8xCGDSccvEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sejal-godbole/NLP/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and Import nltk library"
      ],
      "metadata": {
        "id": "tsXuPs1ZfFdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k5b4RMqnavig"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjmKM1cndwnm",
        "outputId": "c25b6af9-9568-430c-fb6f-48ae06be6939"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is amazing! I'm learning NLP, tokenization, stemming & lemmatization. #AI #ML\""
      ],
      "metadata": {
        "id": "6EdwHy1wd1UN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Techniques"
      ],
      "metadata": {
        "id": "kvfydllIfNgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whitespace Tokenization"
      ],
      "metadata": {
        "id": "c3f0M-dPfTxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHIAvo0Qd7J4",
        "outputId": "794c31fb-9675-4f9d-9cb0-fc90e2da1ed3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['NLTK', 'is', 'amazing!', \"I'm\", 'learning', 'NLP,', 'tokenization,', 'stemming', '&', 'lemmatization.', '#AI', '#ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation-based Tokenization"
      ],
      "metadata": {
        "id": "ciEQVA5PfXt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "wp = WordPunctTokenizer()\n",
        "print(\"\\nPunctuation-based Tokenization:\")\n",
        "print(wp.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGS1f1AgeUdX",
        "outputId": "edcb2943-f824-4483-9982-64d816b1292f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'I', \"'\", 'm', 'learning', 'NLP', ',', 'tokenization', ',', 'stemming', '&', 'lemmatization', '.', '#', 'AI', '#', 'ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treebank Tokenization"
      ],
      "metadata": {
        "id": "wRpCV4bOfbyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tb = TreebankWordTokenizer()\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(tb.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYsbqmsBeaL2",
        "outputId": "8875a76b-2495-4050-b688-07b7bfb2a0d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', 'I', \"'m\", 'learning', 'NLP', ',', 'tokenization', ',', 'stemming', '&', 'lemmatization.', '#', 'AI', '#', 'ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Tokenization"
      ],
      "metadata": {
        "id": "V2ohKdp0fe7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tt.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFnFhFAeqnZ",
        "outputId": "7f56fc98-f025-4e38-a5e4-a87107d01fdc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['NLTK', 'is', 'amazing', '!', \"I'm\", 'learning', 'NLP', ',', 'tokenization', ',', 'stemming', '&', 'lemmatization', '.', '#AI', '#ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "L3ElLaP4fic2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "-TdVUqJifmaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\", \"studies\"]\n",
        "\n",
        "print(\"\\nPorter Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"→\", ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0I36QhvexEo",
        "outputId": "3c0e2ee9-ec06-47e6-ba45-0685e5be7da2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmer:\n",
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "studies → studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "gQeTtR_LftNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "for word in words:\n",
        "    print(word, \"→\", ss.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LghE5Gtpe2gJ",
        "outputId": "e86cec3b-0151-4d0d-d290-86f1b1049776"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemmer:\n",
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "studies → studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "ce1zPfGFfxOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "print(\"running →\", lemmatizer.lemmatize(\"running\", pos='v'))\n",
        "print(\"better →\", lemmatizer.lemmatize(\"better\", pos='a'))\n",
        "print(\"flies →\", lemmatizer.lemmatize(\"flies\", pos='n'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imkhJwse5Yo",
        "outputId": "c84bd392-8df5-46fc-918d-b4fbdae60c75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "running → run\n",
            "better → good\n",
            "flies → fly\n"
          ]
        }
      ]
    }
  ]
}